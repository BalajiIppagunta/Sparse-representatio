{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1 Task 2\n",
    "#### Student Name:Balaji Ippagunta\n",
    "#### Student ID: 29876451\n",
    "\n",
    "Date: 15/04/2017\n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* docx (for parsing docx document)\n",
    "* re (for regular expression) \n",
    "* nltk (Natural Language Toolkit,)\n",
    "* nltk.collocations (for finding bigrams)\n",
    "* nltk.tokenize (for tokenization)\n",
    "\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "The task is to extract the unit data from the doucment and create a vocabulary.\n",
    "The final output of the file is vocabulary text file which has all the tokens after stemming and a countVec file which gives the sparse representation of the units\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import re\n",
    "from itertools import chain\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import MWETokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Loading and transformation fo the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is read from the doc file and the last two columns are clubbed to gether and are saved in dictionary with value in the first column as key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple tables in the document we are going to iterate over each table and each row in the table extract all the cells in each row for extracting the data.\n",
    "We are using the variable counter to keep track of the varibale's cell number for using the variable asa key and also to normalise the case sensitivity as per the specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' synopsis  outcomes'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document('29876451.docx')\n",
    "unitdict={}\n",
    "for table in doc.tables:\n",
    "    for row in table.rows:\n",
    "        counter=0\n",
    "        contentList=[]\n",
    "        contentString=\"\"\n",
    "        for cell in row.cells:\n",
    "            if counter==0:\n",
    "                key=cell.text\n",
    "                counter+=1\n",
    "            elif counter==1:\n",
    "                cell.text=cell.text[0].lower() + cell.text[1:]\n",
    "                contentString=contentString+\" \"+cell.text\n",
    "                counter+=1\n",
    "            elif counter==2:\n",
    "                outcomesstr=\"\"\n",
    "                temp=cell.text.split(', \\'')\n",
    "                for ele in temp:\n",
    "                    i=0\n",
    "                    # We are converting the case of the first alphabet in the scentence to a lower case\n",
    "                    while i is not \"change\":\n",
    "                        if ele[i].isalpha():\n",
    "                            if i==0:\n",
    "                                ele=ele[i].lower()+ele[i+1:]\n",
    "                            else:\n",
    "                                ele=ele[0:i-1]+ele[i].lower()+ele[i+1:]\n",
    "                            i=\"change\"\n",
    "                                    \n",
    "                        else:\n",
    "                            i+=1\n",
    "                    outcomesstr=\" \"+ele\n",
    "                    contentString=contentString+\" \"+outcomesstr\n",
    "                unitdict[key]=contentString\n",
    "                contentList=[]\n",
    "                counter=0\n",
    "#Removing the data with Title as key as they are the headings of the table columns \n",
    "unitdict.pop('Title', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for tokenizing the words in the document with the given regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeunitdetails(unitid):\n",
    "     \n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "    tokens = tokenizer.tokenize(unitdict[unitid])\n",
    "    return (unitid, tokens) # return a tupel of unitid and a list of tokens\n",
    "\n",
    "units_tokenized = dict(tokenizeunitdetails(unitid) for unitid in unitdict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now tokenizing the scentences by iterating over the dictionary and are saving the tokens into a new dictionary with unit id as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in units_tokenized.items():\n",
    "    for element in value:\n",
    "        if not (element.isalpha()):\n",
    "            value.remove(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedtokensList = list(chain.from_iterable(units_tokenized.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many unique words we have in the whole file and the lexical diversity (i.e., the average number of times a type apprearing in the collection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  4000 \n",
      "Total number of tokens:  30841 \n",
      "Lexical diversity:  7.71025\n"
     ]
    }
   ],
   "source": [
    "vocab = set(combinedtokensList)\n",
    "lexical_diversity = len(combinedtokensList)/len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(combinedtokensList), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words from the tokens as per the specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "stopwords_set = set(stopwords)\n",
    "stopped_tokens = [w for w in combinedtokensList if w not in stopwords_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing tokens which are less than 3 letters length and also removing the tokens from the dictionary which are in the stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unitid,content in units_tokenized.items():\n",
    "    for token in content:\n",
    "        if token in stopwords_set:\n",
    "            content.remove(token)\n",
    "        \n",
    "for unitid,content in units_tokenized.items():\n",
    "    for token in content:\n",
    "        if len(token)<3:\n",
    "            content.remove(token)\n",
    "            \n",
    "for ele in stopped_tokens:\n",
    "    if len(ele)<3:\n",
    "            stopped_tokens.remove(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the top 200 Bigrams for the given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "bigram_finder.apply_freq_filter(20)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-100 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Retokenizing with bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer = MWETokenizer(top_200_bigrams)\n",
    "colloc_units =  dict((unitid, mwetokenizer.tokenize(content)) for unitid,content in units_tokenized.items())\n",
    "all_words_colloc = list(chain.from_iterable(colloc_units.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in all_words_colloc:\n",
    "    if len(value)<3:\n",
    "        all_words_colloc.remove(value)\n",
    "        #print(value)\n",
    "colloc_voc = list(set(all_words_colloc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating document frequency for all the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict={}\n",
    "for token in all_words_colloc:\n",
    "    freq_dict[token]=0\n",
    "for token in colloc_voc:\n",
    "    for key,value in units_tokenized.items():\n",
    "        if token in value:\n",
    "            freq_dict[token]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the tokens which are with less than 5% frequency and more than 95% frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lessthan5=[]\n",
    "morethan95=[]\n",
    "for key,value in freq_dict.items():\n",
    "    if value<10:\n",
    "        lessthan5.append(key)\n",
    "    elif value >184:\n",
    "        morethan95.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the tokens which are with less than 5% frequency and more than 95% frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in all_words_colloc:\n",
    "    if token in morethan95:\n",
    "        all_words_colloc.remove(token)\n",
    "    elif token in lessthan5:\n",
    "        all_words_colloc.remove(token)\n",
    "final_tokens_set=list(set(all_words_colloc))\n",
    "final_tokens_set.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming the words with the help of Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "final_tokens =[]\n",
    "final_tokens = final_tokens + [stemmer.stem(w) for w in final_tokens_set ]\n",
    "final_tokens.sort()\n",
    "final_tokens=list(set(final_tokens))\n",
    "print(len(final_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the vocab.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29876451_vocab.txt File generated\n"
     ]
    }
   ],
   "source": [
    "integer_index=1\n",
    "vocab_dict={}\n",
    "vocabfile = open(\"29876451_vocab.txt\", \"a\")\n",
    "for value in final_tokens:\n",
    "    vocabfile.write(value+\":\"+str(integer_index)+\"\\n\")\n",
    "    vocab_dict[value]=str(integer_index)\n",
    "    integer_index+=1\n",
    "vocabfile.close()\n",
    "print(\"29876451_vocab.txt File generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the countVec.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29876451_countVec.txt File generated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorfileInput=\"\"\n",
    "for key,value in colloc_units.items():\n",
    "    #print(\"1\")\n",
    "    variable=\"\"\n",
    "    for token in final_tokens:\n",
    "        count=0\n",
    "        #print(\"1\")\n",
    "        for item in value:\n",
    "            if token==item:\n",
    "                count+=1\n",
    "        if count>0:\n",
    "            attribute=vocab_dict[token]\n",
    "            variable=variable+str(attribute)+\":\"+str(count)+\" ,\"\n",
    "    variable=variable[0:len(variable)-1]\n",
    "    vectorfileInput=vectorfileInput+key+\", \"+variable+\"\\n\"\n",
    "    #print(key+\", \"+variable)\n",
    "vectorfile = open(\"29876451_countVec.txt\", \"a\")\n",
    "vectorfile.write(\"\"+vectorfileInput)\n",
    "vectorfile.close()\n",
    "print(\"29876451_countVec.txt File generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
